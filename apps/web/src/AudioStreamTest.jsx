// AudioStreamTest.jsx
// WebSocketçµŒç”±ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ STTãƒ†ã‚¹ãƒˆï¼ˆPhase2.5 - VADåˆ¶å¾¡ï¼‰
import { useState, useRef, useEffect } from 'react';
import { useAuth } from './AuthContext';
import { useUserAISettings } from './hooks/useUserAISettings';
import { VADProcessor } from './audio/VADProcessor';

export default function AudioStreamTest() {
  const [recording, setRecording] = useState(false);
  const [connected, setConnected] = useState(false);
  const [messages, setMessages] = useState([]);
  const [sttReady, setSttReady] = useState(false);
  const [transcriptionText, setTranscriptionText] = useState('');
  const [vadActive, setVadActive] = useState(false);
  const [currentVolume, setCurrentVolume] = useState(0);
  const [isSpeaking, setIsSpeaking] = useState(false);
  
  const wsRef = useRef(null);
  const mediaRecorderRef = useRef(null);
  const vadProcessorRef = useRef(null);
  const audioStreamRef = useRef(null);
  const audioChunksRef = useRef([]);
  
  const { user } = useAuth();
  const { settings, fetchSettings } = useUserAISettings(user);

  // ãƒ¦ãƒ¼ã‚¶ãƒ¼è¨­å®šã‚’è‡ªå‹•å–å¾—
  useEffect(() => {
    if (user) {
      fetchSettings();
    }
  }, [user, fetchSettings]);

  const connectWS = () => {
    if (wsRef.current) return;
    const ws = new window.WebSocket('ws://localhost:3001');
    ws.binaryType = 'arraybuffer';
    wsRef.current = ws;
    ws.onopen = () => {
      setConnected(true);
      setMessages(msgs => [...msgs, '[open] WebSocketæ¥ç¶šæˆåŠŸ']);
    };
    ws.onmessage = (e) => {
      if (typeof e.data === 'string') {
        try {
          const msg = JSON.parse(e.data);
          if (msg.type === 'stt_ack') {
            setSttReady(true);
            setMessages(msgs => [...msgs, '[recv] STTãƒ¢ãƒ¼ãƒ‰æº–å‚™å®Œäº†']);
          } else if (msg.type === 'stt_result') {
            setTranscriptionText(msg.text);
            setMessages(msgs => [...msgs, '[recv] èªè­˜: ' + msg.text]);
          } else if (msg.type === 'error') {
            setMessages(msgs => [...msgs, '[recv] ã‚¨ãƒ©ãƒ¼: ' + msg.message]);
          } else {
            setMessages(msgs => [...msgs, '[recv] ' + e.data]);
          }
        } catch (err) {
          setMessages(msgs => [...msgs, '[recv] ' + e.data]);
        }
      } else {
        setMessages(msgs => [...msgs, '[recv] [binary]']);
      }
    };
    ws.onclose = () => {
      setConnected(false);
      setSttReady(false);
      setMessages(msgs => [...msgs, '[close] åˆ‡æ–­']);
      wsRef.current = null;
    };
    ws.onerror = (err) => {
      setMessages(msgs => [...msgs, '[error] ' + err.message]);
    };
  };

  const startSTTMode = () => {
    if (!connected || !user || !settings?.stt_model) return;
    // stt_startãƒ¡ãƒƒã‚»ãƒ¼ã‚¸é€ä¿¡
    wsRef.current.send(JSON.stringify({
      type: 'stt_start',
      stt_model: settings.stt_model
    }));
    setMessages(msgs => [...msgs, '[req] STTãƒ¢ãƒ¼ãƒ‰é–‹å§‹: ' + settings.stt_model]);
  };

  const startRecording = async () => {
    if (!sttReady) return;
    if (vadActive) return;
    
    try {
      // éŸ³å£°ã‚¹ãƒˆãƒªãƒ¼ãƒ å–å¾—
      const stream = await navigator.mediaDevices.getUserMedia({ 
        audio: {
          echoCancellation: true,
          noiseSuppression: true,
          sampleRate: 16000
        }
      });
      audioStreamRef.current = stream;
      
      // VADProcessoråˆæœŸåŒ–
      vadProcessorRef.current = new VADProcessor({
        volumeThreshold: 0.08,
        speechStartDelay: 150,
        speechEndDelay: 1000,
        minSpeechDuration: 800,
        maxSpeechDuration: 30000,
        debug: true
      });
      
      // VADã‚¤ãƒ™ãƒ³ãƒˆãƒãƒ³ãƒ‰ãƒ©ãƒ¼è¨­å®š
      vadProcessorRef.current.onSpeechStart = () => {
        setIsSpeaking(true);
        startSpeechRecording();
        setMessages(msgs => [...msgs, '[VAD] ç™ºè©±é–‹å§‹æ¤œå‡º']);
      };
      
      vadProcessorRef.current.onSpeechEnd = (duration) => {
        setIsSpeaking(false);
        stopSpeechRecording();
        setMessages(msgs => [...msgs, `[VAD] ç™ºè©±çµ‚äº†æ¤œå‡º (${duration}ms)`]);
      };
      
      vadProcessorRef.current.onVolumeChange = (volume) => {
        setCurrentVolume(volume);
      };
      
      // VADé–‹å§‹
      await vadProcessorRef.current.start(stream);
      setVadActive(true);
      setRecording(true);
      setMessages(msgs => [...msgs, '[VAD] ç™ºè©±åŒºé–“æ¤œå‡ºé–‹å§‹']);
      
    } catch (err) {
      setMessages(msgs => [...msgs, '[error] VADé–‹å§‹å¤±æ•—: ' + err.message]);
    }
  };
  
  const startSpeechRecording = () => {
    if (!audioStreamRef.current) return;
    
    try {
      const mediaRecorder = new window.MediaRecorder(audioStreamRef.current, { 
        mimeType: 'audio/webm' 
      });
      mediaRecorderRef.current = mediaRecorder;
      audioChunksRef.current = [];
      
      mediaRecorder.ondataavailable = (e) => {
        if (e.data.size > 0) {
          audioChunksRef.current.push(e.data);
        }
      };
      
      mediaRecorder.onstop = () => {
        const blob = new Blob(audioChunksRef.current, { type: 'audio/webm' });
        if (wsRef.current && wsRef.current.readyState === 1 && blob.size > 0) {
          blob.arrayBuffer().then(buf => {
            console.log('[client] ç™ºè©±åŒºé–“webmé€ä¿¡: ã‚µã‚¤ã‚º', buf.byteLength);
            wsRef.current.send(buf);
          });
        }
        mediaRecorderRef.current = null;
        audioChunksRef.current = [];
      };
      
      mediaRecorder.start();
      
    } catch (err) {
      setMessages(msgs => [...msgs, '[error] ç™ºè©±éŒ²éŸ³é–‹å§‹å¤±æ•—: ' + err.message]);
    }
  };
  
  const stopSpeechRecording = () => {
    if (mediaRecorderRef.current && mediaRecorderRef.current.state === 'recording') {
      mediaRecorderRef.current.stop();
    }
  };

  const stopRecording = () => {
    // VADåœæ­¢
    if (vadProcessorRef.current) {
      vadProcessorRef.current.stop();
      vadProcessorRef.current = null;
    }
    
    // éŒ²éŸ³åœæ­¢
    if (mediaRecorderRef.current && mediaRecorderRef.current.state === 'recording') {
      mediaRecorderRef.current.stop();
    }
    
    // éŸ³å£°ã‚¹ãƒˆãƒªãƒ¼ãƒ åœæ­¢
    if (audioStreamRef.current) {
      audioStreamRef.current.getTracks().forEach(track => track.stop());
      audioStreamRef.current = null;
    }
    
    setVadActive(false);
    setRecording(false);
    setIsSpeaking(false);
    setCurrentVolume(0);
    setMessages(msgs => [...msgs, '[VAD] ç™ºè©±åŒºé–“æ¤œå‡ºåœæ­¢']);
  };

  const disconnectWS = () => {
    if (wsRef.current) wsRef.current.close();
  };

  // åŒºé–“ä¿å­˜APIãƒ†ã‚¹ãƒˆç”¨ãƒœã‚¿ãƒ³
  const sendSaveRequest = () => {
    if (wsRef.current && wsRef.current.readyState === 1) {
      wsRef.current.send(JSON.stringify({ type: 'save' }));
      setMessages(msgs => [...msgs, '[req] åŒºé–“ä¿å­˜ãƒªã‚¯ã‚¨ã‚¹ãƒˆé€ä¿¡']);
    }
  };

  return (
    <div style={{ border: '1px solid #aaa', padding: 16, margin: 16 }}>
      <h3>Realtime STT Test (Phase2 PoC)</h3>
      <div style={{ marginBottom: 8 }}>
        <b>STT Model:</b> {settings?.stt_model || 'æœªè¨­å®š'} | <b>User:</b> {user?.email || 'æœªãƒ­ã‚°ã‚¤ãƒ³'}
      </div>
      <div>
        <button onClick={connectWS} disabled={connected || recording}>1. Connect</button>
        <button onClick={startSTTMode} disabled={!connected || !user || !settings?.stt_model || sttReady}>2. STT Mode</button>
        <button onClick={startRecording} disabled={!sttReady || recording}>3. VADéŒ²éŸ³é–‹å§‹</button>
        <button onClick={stopRecording} disabled={!recording}>4. VADéŒ²éŸ³åœæ­¢</button>
        <button onClick={disconnectWS} disabled={!connected || recording}>Disconnect</button>
      </div>
      
      {/* VADçŠ¶æ…‹è¡¨ç¤º */}
      {vadActive && (
        <div style={{ marginTop: 12, padding: 8, background: '#f0f8ff', border: '1px solid #4169e1' }}>
          <div><b>VADçŠ¶æ…‹:</b> {isSpeaking ? 'ğŸ—£ï¸ ç™ºè©±ä¸­' : 'ğŸ¤« å¾…æ©Ÿä¸­'}</div>
          <div><b>éŸ³é‡ãƒ¬ãƒ™ãƒ«:</b> 
            <div style={{ 
              width: '200px', 
              height: '10px', 
              background: '#ddd', 
              marginTop: '4px',
              position: 'relative'
            }}>
              <div style={{
                width: `${currentVolume * 100}%`,
                height: '100%',
                background: isSpeaking ? '#ff4444' : '#44ff44',
                transition: 'width 0.1s'
              }}></div>
            </div>
          </div>
        </div>
      )}
      {transcriptionText && (
        <div style={{ marginTop: 12, padding: 8, background: '#e8f5e8', border: '1px solid #4caf50' }}>
          <b>èªè­˜çµæœ:</b> {transcriptionText}
        </div>
      )}
      <div style={{ marginTop: 8 }}>
        <b>Status:</b> {connected ? (sttReady ? (recording ? (vadActive ? 'VADéŒ²éŸ³ä¸­' : 'éŒ²éŸ³ä¸­') : 'STTæº–å‚™å®Œäº†') : 'æ¥ç¶šä¸­') : 'æœªæ¥ç¶š'}
      </div>
      <div style={{ marginTop: 8 }}>
        <b>Log:</b>
        <ul style={{ maxHeight: 120, overflow: 'auto', background: '#f7f7f7', fontSize: 13 }}>
          {messages.map((msg, i) => <li key={i}>{msg}</li>)}
        </ul>
      </div>
    </div>
  );
}
